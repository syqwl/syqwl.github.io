<div class="paper-box">
  <div class="paper-box-image">
    <div>
      <div class="badge">ICML 2026 (Conference Submission)</div>
      <img src="/static/assets/publication1.png" alt="Uni-SLTP" width="100%">
    </div>
  </div>
  <div class="paper-box-text">
    <p>Bridging the Gap Between Semantics and Reconstruction: Unifying Sign Language Translation and Production</p>
    <p>Xiao Liu, Shiwei Gan, Yafeng Yin, Jiaxin Yin, <strong>Yaqi Sun</strong>, Bowen Guo, Zhiwei Jiang, Lei Xie, Sanglu Lu</p>
    <p><em>Sign language translation (SLT) and production (SLP) are inverse directions of the sign-text mapping, yet they are typically studied within isolated pipelines. We propose Uni-SLTP, which unifies SLT and SLP under a single conditional sequence modeling view by learning a shared discrete pose token space. To bridge the Semantic-Reconstruction Gap, we introduce SR-RVQ, a hierarchical pose tokenizer that separates a semantic anchor stream from residual motion streams. Experiments on public datasets demonstrate that Uni-SLTP achieves superior SLP motion accuracy while maintaining competitive SLT performance.</em></p>
  </div>
</div>

<div class="paper-box">
  <div class="paper-box-image">
    <div>
      <div class="badge">Conference Submission</div>
      <img src="/static/assets/publication2.png" alt="Rejection-Aware Best-of-N" width="100%">
    </div>
  </div>
  <div class="paper-box-text">
    <p>Rejection-Aware Best-of-N Speculation with Shared Prefix Caching</p>
    <p>Rui Ning, Zuxi Chen, Chao Fang, <strong>Yaqi Sun</strong>, Xue Li, Zhibin Wang, Rong Gu, Sheng Zhong, Chen Tian</p>
    <p><em>We propose a rejection-aware Best-of-N speculation method with shared prefix caching to improve large language model inference efficiency.</em></p>
  </div>
</div>
